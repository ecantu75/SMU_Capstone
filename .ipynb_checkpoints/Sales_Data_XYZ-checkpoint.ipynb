{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp1\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import folium\n",
    "from sklearn import metrics\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to load all the files from a specific folder\n",
    "def loadFiles(myPath):\n",
    "    a=1\n",
    "    salesFilesPath = [x for x in os.listdir(myPath)]\n",
    "    \n",
    "    for dataFile in salesFilesPath:\n",
    "        print(\"Loading file: \" + dataFile)\n",
    "        tmpSales_df = pd.read_csv(myPath + \"\\\\\" + dataFile,low_memory=False)\n",
    "        if a == 1 :\n",
    "            sales_df = tmpSales_df\n",
    "            a=0\n",
    "            #print(\"Stored\")\n",
    "        else:\n",
    "            sales_df = sales_df.append(tmpSales_df, ignore_index=True)\n",
    "            #print(\"Append\")\n",
    "    return sales_df\n",
    "    \n",
    "salesTs_df=loadFiles(\"C:\\\\Users\\\\Eduardo Cantu\\\\Documents\\\\Masters\\\\CapstoneA\\\\Data\\\\JupyterNotebooks\\\\SMU_Capstone\\\\Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Analysis\n",
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesTs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features of interest\n",
    "__Ship Date__: This is to be use as the time component  \n",
    "__Price each__: This can be aggregated by date as cashflow or income per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesTs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "_Evaluate if these duplicates are valid or not before deleting them_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print shape before duplicate removal\n",
    "print(\"Data Shape before duplicate Removal:\", salesTs_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select duplicate rows except first occurrence based on all columns\n",
    "duplicateRowsDF = salesTs_df[salesTs_df.duplicated()]\n",
    " \n",
    "print(\"Duplicate Rows except first occurrence based on all columns are :\")\n",
    "print(duplicateRowsDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section to Delete the duplicates from the main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "#### Categorical Information\n",
    "Observations from the data:\n",
    "    * There are 3922 rows with no date. These rows can be deleted.  \n",
    "    * SRV and FGHO fields can be removed. All the values are False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Categorical Objects\n",
    "list_include = ['object']\n",
    "salesTs_df.describe(include=list_include).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows or observations with date = 00/00/00\n",
    "salesTs_df = salesTs_df[salesTs_df['Ship Date'] != \"00/00/00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Categorical Objects after removing the rows with date = 00/00/00\n",
    "list_include = ['object']\n",
    "salesTs_df.describe(include=list_include).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Data\n",
    "_Evaluate from these data is what would need to be forecast. There are negative values on prices should these be deleted?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Float Objects\n",
    "list_include = ['float64','int64']\n",
    "salesTs_df.describe(include=list_include).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all float values\n",
    "list_include = ['float64','int64']\n",
    "i=1\n",
    "for col in salesTs_df.select_dtypes(include=list_include).columns:\n",
    "    if i<=3:\n",
    "        #plt.figure(figsize=(10,4))\n",
    "        plt.subplot(1,3,i)\n",
    "        plt.scatter(range(salesTs_df.shape[0]),np.sort(salesTs_df[col].values))\n",
    "        plt.xlabel('Index', size=10)\n",
    "        plt.title(col, size=10)\n",
    "        plt.ylabel('Value', size=10)\n",
    "        plt.xticks(size=7)\n",
    "        i = i + 1\n",
    "    else:\n",
    "                \n",
    "        plt.subplots_adjust(top=1, bottom=0.4, left=0.01, right=1.8, hspace=0.25, wspace=2)\n",
    "        plt.show()\n",
    "        \n",
    "        i=1\n",
    "        plt.subplot(1,3,i)\n",
    "        plt.scatter(range(salesTs_df.shape[0]),np.sort(salesTs_df[col].values))\n",
    "        plt.xlabel('Index', size=10)\n",
    "        plt.title(col, size=10)\n",
    "        plt.ylabel('Value', size=10)\n",
    "        plt.xticks(size=7)\n",
    "        i = i + 1\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "plt.subplots_adjust(top=1, bottom=0.4, left=0.01, right=1.8, hspace=0.25, wspace=2)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "\n",
    "It can be observed that there is a positve correation between _Price each_ and _MaterialCost_. All other variables seem to be independent of each other.  \n",
    "There there are some outliers in the data that may skew some of the results if used as part of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 - Read the dataset, calculate column correlations and make a seaborn heatmap\n",
    "\n",
    "data = salesTs_df.copy()\n",
    "\n",
    "corr = data.corr()\n",
    "ax = seaborn.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=seaborn.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Matrix\n",
    "This scatter matrix shows in many cases that the variables are independent of each other. This is in line with the previous heat map. However, there is a need to understand if some of these variables can be the basis for a new feature. For example costs, can be the addition of _Freight, MaterialCosts, and Labor_ if this were to be of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic correlogram\n",
    "data=data.dropna(how='all')\n",
    "seaborn.pairplot(data[['FGHO Mat Cost','Freight','Qty','Price each', 'MaterialCost','Labor']], diag_kind=None)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "The histograms show large outliers in each of the variables. The majority of the values are concentrated in the hundred range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all float values\n",
    "list_include = ['float64','int64']\n",
    "i=1\n",
    "for col in salesTs_df.select_dtypes(include=list_include).columns:\n",
    "    if i<=3:\n",
    "        #plt.figure(figsize=(10,4))\n",
    "        plt.subplot(1,3,i)\n",
    "        plt.hist(x=salesTs_df[col].dropna().values, bins=100)\n",
    "        plt.xlabel('Value', size=10)\n",
    "        plt.title(col, size=10)\n",
    "        plt.ylabel('Frequency', size=10)\n",
    "        plt.xticks(size=7)\n",
    "        i = i + 1\n",
    "    else:\n",
    "                \n",
    "        plt.subplots_adjust(top=1, bottom=0.4, left=0.01, right=1.8, hspace=0.25, wspace=2)\n",
    "        plt.show()\n",
    "        \n",
    "        i=1\n",
    "        plt.subplot(1,3,i)\n",
    "        plt.hist(x=salesTs_df[col].dropna().values, bins=100)\n",
    "        plt.xlabel('Value', size=10)\n",
    "        plt.title(col, size=10)\n",
    "        plt.ylabel('Frequency', size=10)\n",
    "        plt.xticks(size=7)\n",
    "        i = i + 1\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "plt.subplots_adjust(top=1, bottom=0.4, left=0.01, right=1.8, hspace=0.25, wspace=2)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "There are no considerable number of missing values in the _Ship Date_, _Price each_,or _Qty_ columns that would need estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "dtype_df=salesTs_df.dtypes.reset_index()\n",
    "dtype_df.columns=[\"Count\",\"ColumnType\"]\n",
    "dtype_df.groupby(\"ColumnType\").aggregate('count').reset_index()\n",
    "missing_df=salesTs_df.isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns=['ColumnName','MissingCount']\n",
    "missing_df=missing_df.ix[missing_df['MissingCount']>0]\n",
    "missing_df=missing_df.sort_values(by='MissingCount')\n",
    "\n",
    "missing_df = missing_df.merge(dtype_df,left_on = 'ColumnName', right_on = 'Count', how= 'left')\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Summary  \n",
    "This section will explore the trend in sales, material quantities, and orders count. There are some observation that need to be remove along the analysis due wrong dates and negative sales. \n",
    "_The removed records would need to be validated_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order Sum by day\n",
    "\n",
    "## Time Series for Price each and Qty\n",
    "\n",
    "priceTs_df = salesTs_df[['Ship Date',\n",
    "                        'Price each',\n",
    "                        'Qty']]\n",
    "\n",
    "priceTs_df['Ship Date'] =  pd.to_datetime(priceTs_df['Ship Date'])\n",
    "\n",
    "order_sum_df=priceTs_df.groupby(['Ship Date']).agg(['sum','count']).reset_index().sort_values(by=(['Ship Date']))\n",
    "sales_sum_df=order_sum_df.set_index('Ship Date')\n",
    "\n",
    "sales_sum_df['Year'] = sales_sum_df.index.year\n",
    "sales_sum_df['Month'] = sales_sum_df.index.month\n",
    "sales_sum_df['WeekdayName'] = sales_sum_df.index.weekday_name\n",
    "sales_sum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove dates outside the range provided in the data\n",
    "sales_sum_df = sales_sum_df[(sales_sum_df.index >= '2009-1-1') & (sales_sum_df.index <= '2018-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set(rc={'figure.figsize':(17, 10)})\n",
    "sales_sum_df['Price each','sum'].plot(linewidth=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sales_sum_df['Price each'].plot(y='count', legend=False)\n",
    "ax2 = ax.twinx()\n",
    "sales_sum_df['Price each'].plot(y=\"sum\", ax=ax2, legend=False, color=\"g\")\n",
    "ax.figure.legend()\n",
    "ax.set_ylabel('Order Count');\n",
    "ax2.set_ylabel('Sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sales_sum_df.loc['2012','Price each'].plot(y='count', legend=False)\n",
    "ax2 = ax.twinx()\n",
    "sales_sum_df.loc['2012','Price each'].plot(y='sum', ax=ax2, legend=False, color=\"g\")\n",
    "ax.figure.legend()\n",
    "ax.set_ylabel('Sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sum_df[sales_sum_df['Price each','sum'] > 300000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sum_df[sales_sum_df['Price each','sum'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the dates with issues. Given the case that these records are valid, then this piece of code should be removed\n",
    "sales_sum_df=sales_sum_df[sales_sum_df['Price each','sum'] < 300000]\n",
    "\n",
    "sales_sum_df=sales_sum_df[sales_sum_df['Price each','sum'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot - Total of Sales and Number of orders\n",
    "\n",
    "This plot is difficult to follow since are daily values. It can still be seen that the trends follow each other. Monthly aggregate plots can give a better sense of how these two variables follow each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sales_sum_df['Price each'].plot(y='count', legend=False)\n",
    "ax2 = ax.twinx()\n",
    "sales_sum_df['Price each'].plot(y=\"sum\", ax=ax2, legend=False, color=\"g\")\n",
    "ax.figure.legend()\n",
    "ax.set_ylabel('Order Count');\n",
    "ax2.set_ylabel('Sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Quantity plot vs Total Orders**  \n",
    "_The plot shows negative quantities. Does this means that these are returns?_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sales_sum_df['Qty'].plot(y='count', legend=False)\n",
    "ax2 = ax.twinx()\n",
    "sales_sum_df['Qty'].plot(y=\"sum\", ax=ax2, legend=False, color=\"g\")\n",
    "ax.figure.legend()\n",
    "ax.set_ylabel('Order Count');\n",
    "ax2.set_ylabel('Total Qty');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between Price and Quantity\n",
    "\n",
    "The Price and Quantity variables are positive correlated. The more Quantity is sold the more the total price or revenue. Also,  the correlation between order count and revenue is present as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic correlogram\n",
    "data=sales_sum_df.dropna(how='all')\n",
    "seaborn.pairplot(data[['Qty','Price each']], diag_kind='hist', kind='reg')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantity and Sales over time  \n",
    "\n",
    "The quantities trend starts a downtrend before reaching around 2011. However, the sales remain stable during this downward trend in quantities, which may suggest lower quantities of more expensive packaging sales.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sales_sum_df['Price each'].resample('MS').sum()\n",
    "y2=sales_sum_df['Qty'].resample('MS').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y.plot(y='sum', legend=False)\n",
    "ax2 = ax.twinx()\n",
    "y2.plot(y=\"sum\", ax=ax2, legend=False, color=\"g\")\n",
    "ax.figure.legend()\n",
    "ax.set_ylabel('Sales');\n",
    "ax2.set_ylabel('Total Quantity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition of the trend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is for the split for the monthly total orders  \n",
    "The number of orders reached its peak around mid-2011, since then there has been a downward trend in the number of orders of materials for packaging, reaching its lowest point on mid-2015, since then it has to a 50 % of what was in 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 18, 8\n",
    "decomposition = sm.tsa.seasonal_decompose(y['count'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split for the Sales Variable (Price each)\n",
    "The sales revenue peaked at the end of 2011. During next three years and a half the trend remained flat until mid-2014 when the sales revenue started to tren down, it reached its lowest point at the end of 2015, and the recovery has been slow, but constant. However, the number of orders has maintained constant, which indicates an increase in prices or the orders includes sophisticated, more expensive packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rcParams['figure.figsize'] = 18, 8\n",
    "decomposition = sm.tsa.seasonal_decompose(y['sum'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the split for the total monthly quantities\n",
    "The monthly quantities showed a downward trend from 2011 until mid-2015 when the trend started to pick up until the end of 2016. Since 2016 the monthly quantities trend has been flat. Also, the residual or random component has maintained the variability under control since 2016.\n",
    "On the seasonality shows a decrease in quantities at the end of each year and picking up the highest number of material in the first and second quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 18, 8\n",
    "decomposition = sm.tsa.seasonal_decompose(y2['sum'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
